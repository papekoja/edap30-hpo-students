{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-19T09:25:06.462154Z",
     "start_time": "2023-04-19T09:25:05.043600Z"
    },
    "tags": []
   },
   "source": [
    "# Hyperparameter optimization with Bayesian optimization\n",
    "\n",
    "Bayesian optimization (BO) is a tool to optimize black-box function, i.e., functions with unknown structure that are expensive to evaluate.\n",
    "BO consists of the following steps that are repeated until one runs out of time or money:\n",
    "\n",
    "1. Learn a model of the function\n",
    "2. Using this model, find a promising point to evaluate next\n",
    "3. Evaluate the point\n",
    "4. Refine the model using the new observation\n",
    "\n",
    "In this lab, we will first learn the BO concepts on a simple example and then transfer this to a real-world hyperparameter optimization task. \n",
    "We will use Gaussian processes (GPs) as surrogate models.\n",
    "Together with random forests (RFs), GPs are the most popular type of surrogate model.\n",
    "Generally, a surrogate model for BO must provide an uncertainty estimate, i.e., for a given point, the model should be able to say how certain it is in its prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-08T11:57:53.486361550Z",
     "start_time": "2023-05-08T11:57:53.481667670Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We first define a synthetic benchmark function we will work on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-08T11:54:02.793540907Z",
     "start_time": "2023-05-08T11:54:02.567525961Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fn(\n",
    "        x: np.ndarray\n",
    ") -> np.ndarray:\n",
    "    y = np.sin(x) + np.sin((10.0 / 3.0) * x)\n",
    "    return y\n",
    "\n",
    "\n",
    "# define lower and upper bounds of the function: we will only search for the optimum within these bounds\n",
    "lb, ub = -2.7, 7.5\n",
    "\n",
    "plt.plot(np.linspace(lb, ub, 200), fn(np.linspace(lb, ub, 200)), label='f(x)')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Defining the training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-08T11:54:02.849967898Z",
     "start_time": "2023-05-08T11:54:02.795115052Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "x_train = np.random.RandomState(1).rand(5) * (ub - lb) + lb\n",
    "x_train = x_train[:, np.newaxis]\n",
    "y_train = fn(x_train)\n",
    "\n",
    "x_test = np.linspace(lb, ub, 250)[:, np.newaxis]\n",
    "y_test = fn(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing a Gaussian process\n",
    "\n",
    "A GP can be seen as a probability distribution over _functions_.\n",
    "One can draw functions from a GP and given some observations, one can update the probability distribution using Bayes' rule.\n",
    "In particular, we assume that our function $f$ is distributed according to a GP:\n",
    "\n",
    "$$\n",
    "f(\\mathbf{x}) \\sim \\mathcal{GP}(\\mu(\\mathbf{x}), k(\\mathbf{x},\\mathbf{x'}))\n",
    "$$\n",
    "\n",
    "$\\mu$ is the _mean function_ and $k$ is the _kernel_ or _covariance function_. \n",
    "The mean function describes the average value of a point and the covariance function essentially captures how much two points are, on average, correlated.\n",
    "\n",
    "In our example, we will use the RBF kernel:\n",
    "\n",
    "$$\n",
    " k(\\mathbf{x},\\mathbf{x'}) = \\exp\\left(-\\frac{||\\mathbf{x}-\\mathbf{x}'||^2}{2l^2}\\right).\n",
    "$$\n",
    "\n",
    "This kernel-function has a larger value the closer $\\mathbf{x}$ and $\\mathbf{x'}$ are to each other, i.e., it assumes that the relation of two points is only determined by their distance. \n",
    "There are other kernels whose value depends on the location of the points but we will only work with kernels that have this \"distance\" assumption.\n",
    "The parameter $l$ is a hyperparameter of the model.\n",
    "We will see later how it affects the GP and we will see how it can be learned from the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining an RBF kernel with a global lengthscale\n",
    "\n",
    "**Your task:** Implement the RBF kernel as described above.\n",
    "\n",
    "**Question:** What does a value of 1 mean, what does a value of 0 mean?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-08T11:54:02.850280600Z",
     "start_time": "2023-05-08T11:54:02.838935547Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def kern(x: np.ndarray, y: np.ndarray, ls: float) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Returns the covariance matrix for x and x'. \n",
    "    \n",
    "    Note: the output should always be a 2d matrix. If x or x' are 1d vectors (with d elements), first reshape them to (1,d)-matrices.\n",
    "    If x is of shape (n,d) and x' is of shape (m,d), the output will be a (n x m) matrix.\n",
    "    \"\"\"\n",
    "\n",
    "    # ‚û°Ô∏è TODO : implement the RBF kernel ‚¨ÖÔ∏è\n",
    "\n",
    "    return ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Safety check\n",
    "\n",
    "The cell below should display the following image:\n",
    "\n",
    "![RBF kernel plot](kernel_plot.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-08T11:54:03.312165140Z",
     "start_time": "2023-05-08T11:54:02.839126389Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "GRANULARITY = 50\n",
    "\n",
    "x = np.linspace(-2, 2, GRANULARITY).reshape(-1, 1)\n",
    "y = np.linspace(-2, 2, GRANULARITY).reshape(-1, 1)\n",
    "\n",
    "xlabels = [''] * GRANULARITY\n",
    "ylabels = [''] * GRANULARITY\n",
    "\n",
    "for idx in np.arange(GRANULARITY)[::5]:\n",
    "    xlabels[idx] = f\"{x.squeeze()[idx]:.2f}\"\n",
    "    ylabels[idx] = f\"{y.squeeze()[idx]:.2f}\"\n",
    "\n",
    "z = kern(x, y, 0.5)\n",
    "sns.heatmap(z, xticklabels=xlabels, yticklabels=ylabels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining a mean function (constant zero)\n",
    "\n",
    "Surprisingly, the mean function is not really important. \n",
    "All we are interested in is the posterior of the GP, i.e., the distribution over functions after we have seen some data.\n",
    "We will see that the prior mean (which is the mean function here) is unimportant for that.\n",
    "Therefore, we can just implement the mean function as constant zero.\n",
    "\n",
    "**Your task:** Implement the mean function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-08T11:54:03.317176932Z",
     "start_time": "2023-05-08T11:54:03.314652945Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def mean(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Returns the mean. \n",
    "    \n",
    "    If x is a 1d vector with d elements, reshape it to a (1 x d) matrix first.\n",
    "    Should return a 2d zeros vector.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    # ‚û°Ô∏è TODO : implement the mean function ‚¨ÖÔ∏è\n",
    "\n",
    "    return ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Safety check\n",
    "\n",
    "The output of the following cell should be __exactly__ `array([[0.]]) (1, 1)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-08T11:54:03.411050475Z",
     "start_time": "2023-05-08T11:54:03.318046991Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(mean(np.zeros((1, 4))), mean(np.zeros(4)).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple GP implementation\n",
    "\n",
    "We will now use the mean and covariance functions to build a GP. \n",
    "In the `__call__` function, we will compute the posterior distribution for a vector of points `x`. The output will __not__ be an array, but a __multivariate Gaussian__ with the dimensionality of the length of `x`.\n",
    "\n",
    "The posterior mean $\\mu_n$ of a GP is calculated as follows:\n",
    "$$\n",
    "\\mu_n(\\mathbf{x}) = \\Sigma(\\mathbf{x},\\mathbf{x}_{1:n})\\Sigma(\\mathbf{x}_{1:n},\\mathbf{x}_{1:n})^{-1}(f(\\mathbf{x}_{1:n})-\\mu(\\mathbf{x}_{1:n}))+\\mu(\\mathbf{x})\n",
    "$$\n",
    "where $\\Sigma_{i,j}=k(\\mathbf{x}_i,\\mathbf{x}_j)$ is the kernel matrix or Gram matrix. $\\mathbf{x}_{1:n}$ is the training data for which we have observed function values and $\\mathbf{x}$ is the point for which we want to make predictions.\n",
    "\n",
    "The posterior variance is calculated as \n",
    "$$\n",
    "\\sigma_n(\\mathbf{x}) = k(\\mathbf{x},\\mathbf{x})-\\Sigma_0(\\mathbf{x},\\mathbf{x}_{1:n})\\Sigma(\\mathbf{x}_{1:n},\\mathbf{x}_{1:n})^{-1}\\Sigma_0(\\mathbf{x},\\mathbf{x}_{1:n})^{\\intercal}\n",
    "$$\n",
    "\n",
    "In the `fit`  function, we will set the training data and calculate a critical component: the inverse of the covariance matrix $\\Sigma(\\mathbf{x}_{1:n},\\mathbf{x}_{1:n})^{-1}$.\n",
    "Usually, this is done using Cholesky decomposition but we will use `np.linalg.inv` to calculate the inverse.\n",
    "\n",
    "Pro tip: You might run into problems when computing the inverse due to numerical instabilities. If this happens, add a value of $10^{-6}$ to the diagonal elements of the matrix before computing the inverse.\n",
    "\n",
    "**Your task:** Set the training data and compute the inverse of the covariance matrix $\\Sigma(\\mathbf{x}_{1:n},\\mathbf{x}_{1:n})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-08T11:54:03.426815886Z",
     "start_time": "2023-05-08T11:54:03.363421012Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class GaussianProcess:\n",
    "    def __init__(\n",
    "            self,\n",
    "            mean_function: Callable[[np.ndarray], np.ndarray],\n",
    "            kern_function: Callable[[np.ndarray, np.ndarray], np.ndarray],\n",
    "            ls: float\n",
    "    ):\n",
    "        # setting the mean function\n",
    "        self.mean = mean_function\n",
    "        # setting the kernel function, fix the lengthscale so we don't have to pass it all the time\n",
    "        self.kern = lambda x, y: kern_function(x, y, ls)\n",
    "        self.x_train = None\n",
    "        self.y_train = None\n",
    "\n",
    "    def initialize(self, x_train: np.ndarray, y_train: np.ndarray) -> 'GaussianProcess':\n",
    "        # some checks\n",
    "        assert x_train.ndim == 2 and y_train.ndim == 2, 'x_train and y_train should be 2D arrays'\n",
    "        assert x_train.shape[0] == y_train.shape[0], 'first dimension of x_train and y_train has to be equal (n_points)'\n",
    "        assert y_train.shape[1] == 1, 'y_train has to be of form (n_points, 1)'\n",
    "        self.x_train = x_train\n",
    "        self.y_train = y_train\n",
    "        # Due to numerical instabilities, the covariance matrix might not be invertible.\n",
    "        # We add a small constant value to the diagonal elements ('jitter') to enfore the\n",
    "        # matrix to be positive semidefinite (which implies invertibility)\n",
    "        # See, e.g., https://scicomp.stackexchange.com/questions/36342/advantage-of-diagonal-jitter-for-numerical-stability\n",
    "        #\n",
    "        #\n",
    "        # The inputs x_train and y_train both have to be 2D here. x_train has shape (n_points, dim_of_points), y_train has\n",
    "        # shape (n_points, 1)\n",
    "        #\n",
    "        # ‚û°Ô∏è TODO : compute the covariance and the inverse of the covariance matrix here. save them as class attributes so we don't need to\n",
    "        #  recompute them all the time ‚¨ÖÔ∏è\n",
    "        #\n",
    "\n",
    "        return self\n",
    "\n",
    "    def __call__(self, x: np.ndarray) -> sp.stats._multivariate.multivariate_normal_frozen:\n",
    "        assert self.x_train is not None and self.y_train is not None, \"Have to initialize the GP before calling\"\n",
    "        #\n",
    "        # ‚û°Ô∏è TODO : compute the posterior distribution ‚¨ÖÔ∏è\n",
    "        #\n",
    "        # First, compute the posterior mean and covariance (use self.cov_inv and the definitions above), \n",
    "        # then compute the posterior distribution which is a multivariate normal distribution\n",
    "        # Hint: you might need to add a small diagonal value to the covariance matrix again\n",
    "        # Use the code from above for that (don't add more than 1e-6)\n",
    "\n",
    "        if x.ndim == 1:\n",
    "            x = x[np.newaxis, :]\n",
    "        return ...\n",
    "\n",
    "    def posterior_mean(self, x: np.ndarray) -> np.ndarray:\n",
    "        if x.ndim == 1:\n",
    "            x = x[np.newaxis, :]\n",
    "        dist = self(x)\n",
    "        return dist.mean\n",
    "\n",
    "    def posterior_covariance(self, x: np.ndarray) -> np.ndarray:\n",
    "        if x.ndim == 1:\n",
    "            x = x[np.newaxis, :]\n",
    "        dist = self(x)\n",
    "        return dist.cov\n",
    "\n",
    "    def log_marginal_likelihood(\n",
    "            self,\n",
    "    ) -> float:\n",
    "        #\n",
    "        # ‚û°Ô∏è TODO : Implement this when you're instructed to in the notebook, you can skip this for now otherwise ‚¨ÖÔ∏è\n",
    "        # \n",
    "        return ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Safety check:** Run the following cell to make sure that everything is working correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-08T11:57:02.658731126Z",
     "start_time": "2023-05-08T11:57:02.579452197Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if isinstance(\n",
    "        GaussianProcess(mean, kern, np.random.rand()).initialize(np.random.rand(3, 1), np.random.rand(3, 1))(\n",
    "            np.random.rand(3, 1)\n",
    "        ),\n",
    "        sp.stats._multivariate.multivariate_normal_frozen\n",
    "):\n",
    "    print(\"‚úÖ All good.\")\n",
    "else:\n",
    "    print(\"üö® __call__ does not return a multivariate distribution. Make sure to use 'sp.stats.multivariate_normal'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "source": [
    "## Initializing and fitting the Gaussian process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-08T11:54:03.451831504Z",
     "start_time": "2023-05-08T11:54:03.363909558Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#\n",
    "# ‚û°Ô∏è TODO : Create and initialize a new GP instance with a lengthscale of your choice. Try different lengthscales to see how the affect the behavior of the GP.  ‚¨ÖÔ∏è\n",
    "#       \n",
    "gp = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-08T11:54:03.452056958Z",
     "start_time": "2023-05-08T11:54:03.364179772Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_gp(gp: GaussianProcess, x_test: np.ndarray, y_test: np.ndarray, ax: plt.Axes = None) -> None:\n",
    "    \"\"\"\n",
    "    Plot the GP posterior distribution of gp for a given set of test points (x_test and y_test).\n",
    "    Accepts an optional parameter ax which plots it on an existing ax, otherwise a new figure\n",
    "    is create.\n",
    "    \"\"\"\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(1, 1)\n",
    "    posterior_distribution = gp(x_test)\n",
    "    rvs = posterior_distribution.rvs(10)\n",
    "\n",
    "    ci_lb = []\n",
    "    ci_ub = []\n",
    "\n",
    "    for _x in x_test:\n",
    "        x_marginal = gp(_x)\n",
    "        _mean = x_marginal.mean.squeeze()\n",
    "        variance = x_marginal.cov.squeeze()\n",
    "\n",
    "        _lb, _ub = sp.stats.norm.interval(0.95, loc=_mean, scale=np.sqrt(variance))\n",
    "        ci_lb.append(_lb)\n",
    "        ci_ub.append(_ub)\n",
    "\n",
    "    ci_lb = np.array(ci_lb)\n",
    "    ci_ub = np.array(ci_ub)\n",
    "\n",
    "    for i, rv in enumerate(rvs):\n",
    "        x_test_sq = x_test.squeeze()\n",
    "        ax.plot(x_test.squeeze(), rv, alpha=0.25, color='blue', label='posterior sample' if i == 0 else None)\n",
    "    ax.fill_between(x_test.squeeze(), ci_lb, ci_ub, color='gray', alpha=0.5, label='95% CI')\n",
    "    ax.plot(x_test, y_test, color='red', label='f(x)')\n",
    "    ax.scatter(x_train, y_train, marker='x', color='black', label='training data')\n",
    "    ax.set_ylabel('f(x)')\n",
    "    ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the above GP for the test data defined earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-08T11:54:03.941063193Z",
     "start_time": "2023-05-08T11:54:03.406881598Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_gp(gp, x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task: play around with lengthscales\n",
    "\n",
    "* What happens if too low?\n",
    "* What happens if too high?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Systematic approach to fit lengthscales: MLE\n",
    "\n",
    "We saw above that setting the lengthscale incorrectly can render the Gaussian process useless: if it is set too low or too high, the Gaussian process fails in modelling the function reasonably (if you didn't see that, try lengthscales of 0.1 and 10).\n",
    "While you probably found a lengthscale value that led to reasonable performance, we want to set the lengthscale automatically because what a \"good value\" is depends on the function at hand.\n",
    "All approaches to set the lengthscale (and often more hyperparameters) are in some form **maximizing the marginal likelihood of seeing the training data under the GP prior**:\n",
    "$$\n",
    "p(\\mathbf{y}|X,l) = \\int p(\\mathbf{y}|\\mathbf{f},X,l)p(\\mathbf{f}|X,l)d\\mathbf{f}\n",
    "$$\n",
    "We use the term _marginal_ likelihood because we are marginalizing over \"functions\" $\\mathbf{f}$.\n",
    "Note that $\\mathbf{f}$ is a $n$-dimensional vector which represents one possible realization of the function $f$ at the locations $X \\in \\mathbb{R}^{n\\times d}$.\n",
    "The vector $\\mathbf{f}|X$ follows a multivariate normal distribution, i.e., we marginalize over a normal distribution.\n",
    "\n",
    "We use the most straightforward way of learning the lengthscale: maximum likelihood estimation (MLE):\n",
    "$$\n",
    "\\hat{l} = \\arg\\max_l p(\\mathbf{y}|X,l)\n",
    "$$\n",
    "\n",
    "We state the posterior (log) marginal likelihood here and refer to [Gaussian Processes for Machine Learning](https://gaussianprocess.org/gpml/chapters/RW.pdf) for a derivation:\n",
    "$$\n",
    "\\log p(\\mathbf{f}|X,l) = -\\frac{1}{2}\\mathbf{f}^\\intercal K^{-1}\\mathbf{f}-\\frac{1}{2}\\log |K| -\\frac{n}{2}\\log 2\\pi\n",
    "$$\n",
    "\n",
    "\n",
    "**YOUR TASK**: Implement the `log_marginal_likelihood` in the `GP` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-08T11:54:03.941274568Z",
     "start_time": "2023-05-08T11:54:03.936674720Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We define a function that we can pass to the minimize function from scikit learn\n",
    "\n",
    "def negative_marginal_log_likelihood(ls: float) -> float:\n",
    "    gp = GaussianProcess(mean, kern, ls)\n",
    "    gp.initialize(x_train, y_train)\n",
    "    return -gp.log_marginal_likelihood()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximize the marginal likelihood by minimizing the negative marginal log likelihood\n",
    "\n",
    "**Safety check:** The following cell should print something close to `[0.6862108]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-08T11:54:04.058849379Z",
     "start_time": "2023-05-08T11:54:03.939494104Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "best_ls = sp.optimize.minimize(negative_marginal_log_likelihood, 1)['x']\n",
    "print(best_ls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the GP with lengthscale fitted by MLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-08T11:54:04.627587887Z",
     "start_time": "2023-05-08T11:54:03.989725720Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "gp = GaussianProcess(mean, kern, best_ls)\n",
    "gp.initialize(x_train, y_train)\n",
    "\n",
    "posterior_distribution = gp(x_test)\n",
    "\n",
    "plot_gp(gp, x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Towards Bayesian Optimization: Acquisition functions\n",
    "\n",
    "We now have all the ingredients of the Gaussian process and focus on the actual task: finding the optimizer of a function (i.e., the point with the best function value).\n",
    "The general strategy is to first fit a Gaussian process on a small number of initial training points and then find the next point to evaluate.\n",
    "\n",
    "We find this next point by maximizing a so-called _acquisition function_.\n",
    "While it may seem strange to solve an optimization problem (maximizing the acquisition function) to maximize our black-box function, the acquisition function can be maximized using gradient-based approaches because it has a closed-form expression.\n",
    "\n",
    "We will work with a popular acquisition function: __Expected improvement (EI)__.\n",
    "EI describes by how much, in expectation w.r.t. to our GP posterior after $n$ observations, a given point improves over the current best function value $y^*_n$:\n",
    "$$\n",
    "EI_n(x) = \\mathbb{E}_{f\\sim GP(X,\\mathbf{y})}\\left[|f(x)-y^*_n| \\right]\n",
    "$$\n",
    "EI naturally does an _exploration-exploitation tradeoff_: points that have already been evaluated get zero EI (in noiseless models as in this labs) but regions where all possible functions have a value worse than the current best point also get zero EI. \n",
    "The sweet spot are regions that are under-explored but are also promising.\n",
    "\n",
    "Since our GP posterior follows a multivariate normal distribution, we can find a [closed form expression for the expected improvement](https://arxiv.org/pdf/1807.02811.pdf):\n",
    "$$\n",
    "EI_n(x) = \\left [\\Delta_n(x) \\right ]^+ + \\sigma_n(x)\\varphi \\left ( \\frac{\\Delta_n(x)}{\\sigma_n(x)} \\right ) - |\\Delta_n(x)|\\Phi \\left ( \\frac{\\Delta_n(x)}{\\sigma_n(x)} \\right )\n",
    "$$\n",
    "where $\\Delta_n(x) = \\mu_n(x)-y^*_n$ is the expected difference between the point $x$ and the best function values after $n$ observations $y^*_n$, $\\varphi(x)$ is the probability density function of the normal distribution, $\\Phi$ is the cummulative density function of the normal distribution, and $[x]^+:=\\max(0,x)$.\n",
    "The quantities $\\mu_n$ and $\\sigma_n$ are the posterior mean and posterior standard deviation after $n$ observations.\n",
    "\n",
    "__YOUR TASK:__ Implement the expected improvement acquisition function by adding missing lines in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-08T11:54:04.632256151Z",
     "start_time": "2023-05-08T11:54:04.631077144Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def expected_improvement(gp: GaussianProcess, x: np.ndarray) -> np.ndarray:\n",
    "    if x.ndim == 1:\n",
    "        x = x[np.newaxis, :]\n",
    "    eis = []\n",
    "    for _x in x:\n",
    "        # ‚û°Ô∏è TODO : Implement the expected improvement acquisition function by adding missing lines  ‚¨ÖÔ∏è\n",
    "        _ei = ...\n",
    "        eis.append(_ei.squeeze())\n",
    "    return np.array(eis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Safety check:__ (make sure you executed all cells in order up to here, i.e., you used the GP with MLE). \n",
    "\n",
    "You should get the following figure:\n",
    "\n",
    "![EI plot](EI.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-08T11:54:05.365253473Z",
     "start_time": "2023-05-08T11:54:04.634117019Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.plot(expected_improvement(gp, np.linspace(lb, ub, 300).reshape(-1, 1)).squeeze(), label=rf'$EI_n(x)$')\n",
    "plt.legend()\n",
    "plt.savefig('EI.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian optimization\n",
    "\n",
    "Now, we have all the ingredients to do Bayesian optimization. We will \n",
    "* Sample some initial points and evaluate them\n",
    "* Fit a GP model with MLE\n",
    "* Find the next point to evaluate by maximizing the Expected improvement\n",
    "* Evaluate the next point, add it to the data and start over from the second point \n",
    "\n",
    "We will now see how all components play together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining a function to maximize the likelihood\n",
    "\n",
    "We wrap our MLE procedure in a single function that we can call conveniently later on. Given `mean`, `kern`, `x_train`, and `y_train`, this function returns a GP where the lengthscale is set by MLE. \n",
    "\n",
    "**Question (after running the BO loop):** Why is the model of the function so accurate in the middle part of the function but poor in the outer parts?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-08T11:54:05.365765649Z",
     "start_time": "2023-05-08T11:54:05.360057884Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def maximize_likelihood(\n",
    "        mean: Callable[[np.ndarray], np.ndarray],\n",
    "        kern: Callable[[np.ndarray], np.ndarray],\n",
    "        x_train: np.ndarray, y_train: np.ndarray\n",
    ") -> GaussianProcess:\n",
    "    gp = GaussianProcess(mean, kern, 0.1)\n",
    "    gp.initialize(x_train, y_train)\n",
    "\n",
    "    def likelihood_function(x: np.ndarray) -> float:\n",
    "        gp = GaussianProcess(mean, kern, x)\n",
    "        gp.initialize(x_train, y_train)\n",
    "        return -gp.log_marginal_likelihood()\n",
    "\n",
    "    best_ls = sp.optimize.minimize(likelihood_function, 1)['x']\n",
    "\n",
    "    gp = GaussianProcess(mean, kern, best_ls)\n",
    "    gp.initialize(x_train, y_train)\n",
    "\n",
    "    return gp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-08T11:54:19.081771031Z",
     "start_time": "2023-05-08T11:54:05.368826259Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "x_train = np.random.RandomState(2).rand(5) * (ub - lb) + lb\n",
    "x_train = x_train[:, np.newaxis]\n",
    "y_train = fn(x_train)\n",
    "\n",
    "gp = maximize_likelihood(mean, kern, x_train, y_train)\n",
    "ei = lambda x: expected_improvement(gp, x)\n",
    "\n",
    "for i in range(10):\n",
    "    print('###########################')\n",
    "    print(f\"####### iteration {i} #######\")\n",
    "    print('###########################')\n",
    "\n",
    "    fig, axs = plt.subplots(2, 1, sharex=True, figsize=(7, 7))\n",
    "    # plot current GP\n",
    "    plot_gp(gp, x_test, y_test, ax=axs[0])\n",
    "\n",
    "    # stupidly optimize the acquisition function\n",
    "    x_range = np.linspace(lb, ub, 1000)[:, np.newaxis]\n",
    "    eis = ei(x_range)\n",
    "    x_next = x_range[np.argmax(eis)]\n",
    "\n",
    "    # plot acquisition function\n",
    "    axs[1].plot(x_range, eis)\n",
    "    axs[1].set_ylabel('EI(x)')\n",
    "    axs[1].vlines(x_next, ymin=ei(x_next), ymax=0, color='green', linestyle='dashed', label='best x')\n",
    "    axs[1].legend()\n",
    "    plt.show(fig)\n",
    "\n",
    "    # evaluate point where acquisition function maximum\n",
    "    fx_next = fn(x_next)\n",
    "    # add to training data\n",
    "    x_train = np.vstack((x_train, x_next[np.newaxis, :]))\n",
    "    y_train = np.vstack((y_train, fx_next[np.newaxis, :]))\n",
    "\n",
    "    gp = maximize_likelihood(mean, kern, x_train, y_train)\n",
    "    ei = lambda x: expected_improvement(gp, x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian optimization in action\n",
    "\n",
    "Now, we'll use Bayesian optimization to solve a real-world problem: image classification.\n",
    "We will use the CIFAR10 dataset, which contains 60,000 32x32 color images in 10 classes, with 6,000 images per class. There are 50,000 training images and 10,000 test images.\n",
    "Our goal is to find a model with a high accuracy on the test set.\n",
    "\n",
    "We will use [BoTorch](https://botorch.org/) to optimize the hyperparameters of a convolutional neural network (CNN) using Bayesian optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-08T11:54:20.297792835Z",
     "start_time": "2023-05-08T11:54:19.073102071Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initializing normalizing transform for the dataset\n",
    "\n",
    "import torchvision\n",
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "normalize_transform = torchvision.transforms.Compose(\n",
    "    [\n",
    "        torchvision.transforms.RandomHorizontalFlip(),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize(\n",
    "            mean=(0.5, 0.5, 0.5),\n",
    "            std=(0.5, 0.5, 0.5)\n",
    "        )]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-08T11:54:21.893921914Z",
     "start_time": "2023-05-08T11:54:20.300668225Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "train_dataset = torchvision.datasets.CIFAR10(\n",
    "    root=\"./CIFAR10/train\", train=True,\n",
    "    transform=normalize_transform,\n",
    "    download=True\n",
    ")\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(\n",
    "    root=\"./CIFAR10/test\", train=False,\n",
    "    transform=normalize_transform,\n",
    "    download=True\n",
    ")\n",
    "\n",
    "# Generating data loaders from the corresponding datasets\n",
    "batch_size = 128\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "# Plotting 25 images from the 1st batch\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = next(dataiter)\n",
    "print(images.shape)\n",
    "plt.imshow(\n",
    "    np.transpose(\n",
    "        torchvision.utils.make_grid(\n",
    "            images[:25], normalize=True, padding=1, nrow=5\n",
    "        ).numpy(), (1, 2, 0)\n",
    "    )\n",
    ")\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a simple CNN architecture with 3 convolutional layers and 2 fully-connected layers. We use dropout and spatial dropout layers with a dropout probabilities that are subject to optimization. Furthermore, use LeakyReLU activation functions with a negative slope that we will optimize as a hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-08T11:54:21.896721910Z",
     "start_time": "2023-05-08T11:54:21.894445535Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class CNN(torch.nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            leaky_relu_slope: float,\n",
    "            dropout_p: float,\n",
    "            n_hidden: int\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.model = torch.nn.Sequential(\n",
    "            #Input = 3 x 32 x 32, Output = 32 x 32 x 32\n",
    "            torch.nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1),\n",
    "            torch.nn.LeakyReLU(leaky_relu_slope),\n",
    "            #Input = 32 x 32 x 32, Output = 32 x 16 x 16\n",
    "            torch.nn.MaxPool2d(kernel_size=2),\n",
    "\n",
    "            #Input = 32 x 16 x 16, Output = 64 x 16 x 16\n",
    "            torch.nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1),\n",
    "            torch.nn.LeakyReLU(leaky_relu_slope),\n",
    "            #Input = 64 x 16 x 16, Output = 64 x 8 x 8\n",
    "            torch.nn.MaxPool2d(kernel_size=2),\n",
    "\n",
    "            #Input = 64 x 8 x 8, Output = 64 x 8 x 8\n",
    "            torch.nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1),\n",
    "            torch.nn.LeakyReLU(leaky_relu_slope),\n",
    "            #Input = 64 x 8 x 8, Output = 64 x 4 x 4\n",
    "            torch.nn.MaxPool2d(kernel_size=2),\n",
    "\n",
    "            torch.nn.Flatten(),\n",
    "            torch.nn.Linear(64 * 4 * 4, n_hidden),\n",
    "            torch.nn.Dropout(dropout_p),\n",
    "            torch.nn.LeakyReLU(leaky_relu_slope),\n",
    "            torch.nn.Linear(n_hidden, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the Adam optimizer with a learning rate that is subject to optimization. We will use a weight decay that is subject to optimization. We will use the cross-entropy loss function.\n",
    "\n",
    "We define the training procedure in a function `black_box_function` that takes a vector of hyperparameters as input and returns the accuracy on the test set as output. We will use this function as a black box in the optimization procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-08T11:54:23.422043191Z",
     "start_time": "2023-05-08T11:54:21.897049758Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Selecting the appropriate training device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We wrap the entire training and evaluation loop in a function that we can optimize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-08T11:54:23.436243046Z",
     "start_time": "2023-05-08T11:54:23.434045709Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def black_box_function(x: torch.Tensor) -> float:\n",
    "    x = x.detach().numpy().tolist()\n",
    "    x[1] = x[1]\n",
    "    x[2] = 0.0001 * 10 ** (3 * x[2])\n",
    "    x[3] = 0.001 * 10 ** (3 * x[3])\n",
    "    x[4] = max(1, int(x[4] * 4096))  # max 2048 hidden units\n",
    "    x[5] = max(1, int(x[5] * 10))  # max 10 epochs\n",
    "\n",
    "    leaky_relu_slope = x[0]\n",
    "    dropout_p = x[1]\n",
    "    learning_rate = x[2]\n",
    "    weight_decay = x[3]\n",
    "    n_hidden = x[4]\n",
    "    num_epochs = x[5]\n",
    "\n",
    "    print(f'leaky_relu_slope: {leaky_relu_slope:.3E}', end='\\t')\n",
    "    print(f'dropout_p: {dropout_p:.3E}', end='\\t')\n",
    "    print(f'learning_rate: {learning_rate:.3E}', end='\\t')\n",
    "    print(f'weight_decay: {weight_decay:.3E}')\n",
    "    print(f'n_hidden: {n_hidden}')\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, num_workers=8, shuffle=True)\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, num_workers=8)\n",
    "\n",
    "    model = CNN(leaky_relu_slope, dropout_p, n_hidden).to(device)\n",
    "\n",
    "    #Defining the model hyper parameters\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "    #Training process begins\n",
    "    train_loss_list = []\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = 0\n",
    "\n",
    "        #Iterating over the training dataset in batches\n",
    "        model.train()\n",
    "        for i, (images, labels) in enumerate(train_loader):\n",
    "            #Extracting images and target labels for the batch being iterated\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            #Calculating the model output and the cross entropy loss\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            #Updating weights according to calculated loss\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        #Printing loss for each epoch\n",
    "        train_loss_list.append(train_loss / len(train_loader))\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}: Training loss = {train_loss_list[-1]:.3f}\", end='\\r')\n",
    "\n",
    "        test_acc = 0\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        losses = []\n",
    "        #Iterating over the training dataset in batches\n",
    "        for i, (images, labels) in enumerate(test_loader):\n",
    "            images = images.to(device)\n",
    "            y_true = labels.to(device)\n",
    "\n",
    "            #Calculating outputs for the batch being iterated\n",
    "            outputs = model(images)\n",
    "\n",
    "            losses.append(criterion(outputs, y_true))\n",
    "\n",
    "        test_loss = torch.mean(torch.stack(losses)).detach().cpu()\n",
    "        print(f\"\\nTest loss = {test_loss:.3f}\")\n",
    "\n",
    "        # We maximize the negative test loss, i.e., minimize the loss\n",
    "        return -test_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the GP model\n",
    "\n",
    "We model the GP with GPyTorch. We specify the possible ranges for the hyperparameters (`lengthscale_constraint`, `noise_constraint`, `outputscale_constraint`). The Matern kernel is similar to the RBF kernel we used above. We have not talked about `ScaleKernel`s but they allow the model to adapt to the variance of the values of $f$. Finally, GPs can also model noisy functions. We haven't talked about noise above but the definitions of a noisy GP are almost identical to the ones of noiseless GPs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-08T11:54:23.617987989Z",
     "start_time": "2023-05-08T11:54:23.438063797Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from gpytorch.likelihoods import GaussianLikelihood\n",
    "from gpytorch.constraints import Interval\n",
    "\n",
    "from botorch.models import SingleTaskGP\n",
    "\n",
    "\n",
    "def get_gp(x_train: torch.Tensor, y_train: torch.Tensor) -> SingleTaskGP:\n",
    "    # ‚û°Ô∏è TODO : Create a new SingleTaskGP and return the model  ‚¨ÖÔ∏è\n",
    "    # See https://botorch.org/api/_modules/botorch/models/gp_regression.html#SingleTaskGP for hints\n",
    "    # You may want to constraint the possible noise in the model since our problems are noiseless.\n",
    "    # Use for instance the following likelihood when defining your SingleTaskGP.\n",
    "    likelihood = GaussianLikelihood(noise_constraint=Interval(1e-8, 1e-3))\n",
    "    model = ...\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cheap function to test if everything works\n",
    "\n",
    "You can use the following test function to check if your BO loop runs as intended before running on the expensive HPO problem. On the Branin problem, you should reach a value of around -0.4 after approximately 200 function evaluations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from botorch.test_functions import Branin\n",
    "\n",
    "cheap_function = Branin(negate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a space-filling criterion (\"scrambled Sobol sequence\") to draw the initial points. \n",
    "This basically ensures that we sample evenly from the space, i.e., don't sample more from certain regions of the space than from others.\n",
    "As above, we use Expected Improvement as the acquisition function. We will optimize the acquisition function using the L-BFGS algorithm. The Bayesian optimization loop will run for 24 iterations (and might take some time)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-08T11:56:17.210579536Z",
     "start_time": "2023-05-08T11:54:23.614538186Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from botorch.optim import optimize_acqf\n",
    "from botorch.acquisition import ExpectedImprovement\n",
    "from gpytorch import ExactMarginalLogLikelihood\n",
    "from torch.quasirandom import SobolEngine\n",
    "from botorch.utils.transforms import normalize, unnormalize\n",
    "\n",
    "import torch\n",
    "\n",
    "# ‚û°Ô∏è TODO : Set this to 'cheap_function' or 'black_box_function' ‚¨ÖÔ∏è\n",
    "function_to_optimize = black_box_function\n",
    "dim = 2 if isinstance(function_to_optimize, Branin) else 6\n",
    "\n",
    "# draw d+1 initial points\n",
    "x_init = SobolEngine(dim, scramble=True).draw(dim+1)\n",
    "fx_init = torch.tensor([function_to_optimize(x) for x in x_init]).reshape(-1, 1)\n",
    "\n",
    "x = x_init\n",
    "fx = fx_init\n",
    "\n",
    "# Set to 500 for cheap function and to 24 for the HPO problem.\n",
    "N_BO_STEPS  = 500 if isinstance(function_to_optimize, Branin) else 24\n",
    "# Set to 1 for the HPO problem\n",
    "PRINT_EVERY = 50 if isinstance(function_to_optimize, Branin) else 1\n",
    "\n",
    "print('*** Starting Bayesian Optimization ***')\n",
    "for gp_iter in range(N_BO_STEPS):\n",
    "    if gp_iter % PRINT_EVERY == 0:\n",
    "        print(f'** Iteration {gp_iter + 1}/{N_BO_STEPS} **')\n",
    "    # We define the bounds for the optimization. We assume that all hyperparameters are between 0 and 1.\n",
    "    bounds = torch.stack([torch.zeros(dim), torch.ones(dim)]) if not isinstance(function_to_optimize, Branin) else torch.stack([torch.tensor([-5, 0]), torch.tensor([10,15])])\n",
    "    \n",
    "    # ‚û°Ô∏è TODO : Normalize the x values to be between 0 and 1. You may use the botorch transforms  ‚¨ÖÔ∏è\n",
    "    # https://botorch.org/api/_modules/botorch/utils/transforms.html\n",
    "    x_std = ...\n",
    "    \n",
    "    # ‚û°Ô∏è TODO : Normalize the y values to have mean zero and standard deviation one.  ‚¨ÖÔ∏è\n",
    "    fx_std = ...\n",
    "    \n",
    "    # ‚û°Ô∏è TODO : Create a new GP with the normalized x and y values ‚¨ÖÔ∏è\n",
    "    gp = ...\n",
    "    \n",
    "    # Your GP model has an attribute `likelihood` which you can use to compute the marginal log likelihood.\n",
    "    # This attribute gives the term p(y|f,X,l) in the marginal log likelihood which in our case\n",
    "    # is a Gaussian (see https://docs.gpytorch.ai/en/stable/likelihoods.html#gaussianlikelihood )\n",
    "    # ‚û°Ô∏è TODO : Define the marginal log likelihood of the model (see https://docs.gpytorch.ai/en/stable/marginal_log_likelihoods.html#exactmarginalloglikelihood ) ‚¨ÖÔ∏è\n",
    "    mll = ...\n",
    "    \n",
    "    # ‚û°Ô∏è TODO : Define an acquisition function for your model. We'll use the Expected Improvement (see https://botorch.org/api/_modules/botorch/acquisition/analytic.html#ExpectedImprovement )  ‚¨ÖÔ∏è\n",
    "    ei = ...\n",
    "\n",
    "    # ‚û°Ô∏è TODO : Optimize the acquisition function. You can use the optimize_acqf function (see https://botorch.org/api/optim.html#botorch.optim.optimize.optimize_acqf ) ‚¨ÖÔ∏è\n",
    "    # Set the number of candidates to 1, num restarts to 10, and raw_samples to 1024 Ô∏è\n",
    "    x_next, acq_value = ...\n",
    "    \n",
    "    # ‚û°Ô∏è TODO : Unnormalize x_next to be in the original bounds ‚¨ÖÔ∏è\n",
    "    x_unnorm = ...\n",
    "\n",
    "    # ‚û°Ô∏è TODO : Evaluate your black-box function on the point suggested by the acquisition function ‚¨ÖÔ∏è\n",
    "    fx_next = ...\n",
    "    if gp_iter % PRINT_EVERY == 0:\n",
    "        print(f'New function value: {fx_next.item():.4f}. Current best: {fx.max().item():.4f}')\n",
    "        print('\\n')\n",
    "    x = torch.cat([x, x_unnorm])\n",
    "    fx = torch.cat([fx, fx_next.reshape(-1, 1)])\n",
    "\n",
    "x_bo = x\n",
    "fx_bo = fx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing BO to random search\n",
    "\n",
    "We compare the performance of the models found by BO to randomly searching for CNN hyperparameters. You should see that BO outperforms random search.\n",
    "\n",
    "__Your task:__ Randomly initialize 30 CNNs and evaluate their performance. The performances should be in the 1D tensor `fx_rs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-08T11:56:16.646792551Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ‚û°Ô∏è TODO : Randomly initialize 30 CNNs and evaluate their performance.  ‚¨ÖÔ∏è\n",
    "\n",
    "x_rs = ...\n",
    "# You probably need to write some code here\n",
    "fx_rs = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-08T11:56:16.652955668Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fx_bo_max = np.maximum.accumulate(fx_bo)\n",
    "fx_rs_max = np.maximum.accumulate(fx_rs)\n",
    "\n",
    "plt.plot(fx_bo_max, label='BO')\n",
    "plt.plot(fx_rs_max, label='RS')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-08T11:56:16.659117538Z"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
